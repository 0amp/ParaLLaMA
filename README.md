# parallama
Run large LLaMA with long context length with tensor parallelization support applied to the HF implementation of LLaMA

# ParaLLaMA
Run large LLaMA with long context length with tensor parallelization support applied to the HF implementation of LLaMA

Run ```python parallelization_demo.py``` in conda with the appropriate model and prompt to see it in action! 

**Credits:** Significant portions of the codebase are ported from the [PR](https://github.com/huggingface/transformers/pull/21955) by [Jason Phang](https://github.com/zphang). Inference and training examples are from [user-friendly LLaMA repo](https://github.com/ypeleg/llama) from [Yam Peleg](https://github.com/ypeleg). 
